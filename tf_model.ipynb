{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d717ff",
   "metadata": {},
   "source": [
    "Importing Necessary Liabraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e83015d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33986dae",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f35db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 15 classes: ['Bear', 'Bird', 'Cat', 'Cow', 'Deer', 'Dog', 'Dolphin', 'Elephant', 'Giraffe', 'Horse', 'Kangaroo', 'Lion', 'Panda', 'Tiger', 'Zebra']\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = r\"image folder\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train dataset\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"validation dataset\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"test dataset\")\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "CLASS_NAMES = sorted(os.listdir(TRAIN_DIR))  # Automatically detect classes\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"Detected {NUM_CLASSES} classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9d98d",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20bbbcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1361 files belonging to 15 classes.\n",
      "Found 195 files belonging to 15 classes.\n",
      "Found 388 files belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_dataset(path, subset=None):\n",
    "    try:\n",
    "        return tf.keras.utils.image_dataset_from_directory(\n",
    "            path,\n",
    "            image_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_mode='int',\n",
    "            validation_split=0.2 if subset else None,\n",
    "            subset=subset,\n",
    "            seed=42,\n",
    "            class_names=CLASS_NAMES\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset from {path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load datasets with validation\n",
    "train_ds = load_dataset(TRAIN_DIR)\n",
    "val_ds = load_dataset(VAL_DIR)\n",
    "test_ds = load_dataset(TEST_DIR)\n",
    "\n",
    "if not all([train_ds, val_ds, test_ds]):\n",
    "    raise SystemExit(\"Failed to load datasets. Check directory structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f5b69",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fca02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.3),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def preprocess(image, label):\n",
    "    image = data_augmentation(image)\n",
    "    image = tf.keras.applications.xception.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Optimize dataset pipeline\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "val_ds = val_ds.map(lambda x, y: (tf.keras.applications.xception.preprocess_input(x), y))\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18783b",
   "metadata": {},
   "source": [
    "Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94d48ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    ")\n",
    "\n",
    "# Freeze base model initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build custom head\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc418c9f",
   "metadata": {},
   "source": [
    "Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54937ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1513a1",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c3943fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initial Training ===\n",
      "Epoch 1/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4102 - loss: 1.9540\n",
      "Epoch 1: val_accuracy improved from -inf to 0.92821, saving model to best_model.keras\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.4138 - loss: 1.9416 - val_accuracy: 0.9282 - val_loss: 0.3032\n",
      "Epoch 2/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8273 - loss: 0.5594\n",
      "Epoch 2: val_accuracy improved from 0.92821 to 0.95385, saving model to best_model.keras\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 2s/step - accuracy: 0.8276 - loss: 0.5586 - val_accuracy: 0.9538 - val_loss: 0.1959\n",
      "Epoch 3/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.9044 - loss: 0.3434 \n",
      "Epoch 3: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 10s/step - accuracy: 0.9044 - loss: 0.3431 - val_accuracy: 0.9282 - val_loss: 0.1765\n",
      "Epoch 4/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9443 - loss: 0.2134\n",
      "Epoch 4: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.9445 - loss: 0.2132 - val_accuracy: 0.9333 - val_loss: 0.1490\n",
      "Epoch 5/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9610 - loss: 0.1473\n",
      "Epoch 5: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.9612 - loss: 0.1470 - val_accuracy: 0.9385 - val_loss: 0.1761\n",
      "Epoch 6/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9793 - loss: 0.1063\n",
      "Epoch 6: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 4s/step - accuracy: 0.9794 - loss: 0.1062 - val_accuracy: 0.9231 - val_loss: 0.2319\n",
      "Epoch 7/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9771 - loss: 0.0995\n",
      "Epoch 7: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.9771 - loss: 0.0996 - val_accuracy: 0.9333 - val_loss: 0.2138\n",
      "Epoch 8/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9817 - loss: 0.0796\n",
      "Epoch 8: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 4s/step - accuracy: 0.9817 - loss: 0.0795 - val_accuracy: 0.9333 - val_loss: 0.2126\n",
      "Epoch 9/20\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9846 - loss: 0.0707\n",
      "Epoch 9: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 5s/step - accuracy: 0.9847 - loss: 0.0704 - val_accuracy: 0.9333 - val_loss: 0.2129\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Initial Training ===\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbfdcf",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e064c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tuning ===\n",
      "Epoch 9/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.7993 - loss: 0.7971\n",
      "Epoch 9: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 10s/step - accuracy: 0.8000 - loss: 0.7950 - val_accuracy: 0.9385 - val_loss: 0.1438\n",
      "Epoch 10/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.9450 - loss: 0.3481\n",
      "Epoch 10: val_accuracy did not improve from 0.95385\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 9s/step - accuracy: 0.9452 - loss: 0.3479 - val_accuracy: 0.9538 - val_loss: 0.1505\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Fine-tuning ===\")\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:40]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(1e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7179e5",
   "metadata": {},
   "source": [
    "Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9de9585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "model.save('final_model.keras')\n",
    "print(\"Model saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dafa2b8",
   "metadata": {},
   "source": [
    "Prediction Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89b50e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalClassifier:\n",
    "    def __init__(self, model_path='best_model.keras'):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.class_names = CLASS_NAMES\n",
    "        \n",
    "    def predict(self, image_path):\n",
    "        try:\n",
    "            img = tf.keras.utils.load_img(image_path, target_size=IMG_SIZE)\n",
    "            img_array = tf.keras.utils.img_to_array(img)\n",
    "            img_array = tf.expand_dims(img_array, axis=0)\n",
    "            img_array = tf.keras.applications.xception.preprocess_input(img_array)\n",
    "            \n",
    "            preds = self.model.predict(img_array)\n",
    "            confidence = np.max(preds)\n",
    "            class_index = np.argmax(preds)\n",
    "            \n",
    "            return {\n",
    "                'class': self.class_names[class_index],\n",
    "                'confidence': float(confidence),\n",
    "                'all_predictions': dict(zip(self.class_names, preds[0].tolist()))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6e5f8",
   "metadata": {},
   "source": [
    "Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3ffae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "Test Image: Giraffe_7_4.jpg\n",
      "Actual Class: Giraffe\n",
      "Predicted: Giraffe (71.78%)\n"
     ]
    }
   ],
   "source": [
    "def test_random_prediction():\n",
    "    classifier = AnimalClassifier()\n",
    "    all_images = []\n",
    "    for root, dirs, files in os.walk(VAL_DIR):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                all_images.append(os.path.join(root, file))\n",
    "    \n",
    "    if not all_images:\n",
    "        return \"No images found for testing\"\n",
    "    \n",
    "    test_image = np.random.choice(all_images)\n",
    "    actual_class = os.path.basename(os.path.dirname(test_image))\n",
    "    \n",
    "    result = classifier.predict(test_image)\n",
    "    \n",
    "    print(f\"\\nTest Image: {os.path.basename(test_image)}\")\n",
    "    print(f\"Actual Class: {actual_class}\")\n",
    "    if 'class' in result:\n",
    "        print(f\"Predicted: {result['class']} ({result['confidence']:.2%})\")\n",
    "    else:\n",
    "        print(f\"Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Run test prediction\n",
    "test_random_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa2d9b",
   "metadata": {},
   "source": [
    "Verify model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bc0c282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ xception (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,695</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ xception (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m20,861,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │         \u001b[38;5;34m7,695\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,356,231</span> (237.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m62,356,231\u001b[0m (237.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,218,983</span> (77.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,218,983\u001b[0m (77.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,699,280</span> (6.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,699,280\u001b[0m (6.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,437,968</span> (154.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m40,437,968\u001b[0m (154.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model:\n",
    "    model.summary()\n",
    "\n",
    "# Test with a known image from training set\n",
    "test_image = next(iter(test_ds.take(1)))[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
